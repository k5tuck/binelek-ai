# Server Configuration
API_HOST=0.0.0.0
API_PORT=8096
ENVIRONMENT=development

# LLM Provider (openai, anthropic, or local)
# Defaults to local (Binelek AI) - no API key needed
LLM_PROVIDER=local

# OpenAI Configuration (optional)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4-turbo-preview

# Anthropic Configuration (optional)
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Binelek AI Configuration (Local LLM via Ollama)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:8b

# Default Model (fallback)
LLM_MODEL=binelek-standard
EMBEDDING_MODEL=nomic-embed-text

# Database Connections
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password

QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=

POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=binelek_pipeline
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_postgres_password

# Service URLs
ONTOLOGY_SERVICE_URL=http://localhost:8091
PIPELINE_SERVICE_URL=http://localhost:8094
CONTEXT_SERVICE_URL=http://localhost:8095

# AI Configuration
MAX_REASONING_STEPS=5
TEMPERATURE=0.7
MAX_TOKENS=4000

# Tenant Isolation
ENABLE_TENANT_ISOLATION=true

# Logging
LOG_LEVEL=INFO
